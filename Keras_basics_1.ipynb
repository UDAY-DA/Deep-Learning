{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference \n",
    "# https://www.tensorflow.org/guide/keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n",
      "2.1.6-tf\n"
     ]
    }
   ],
   "source": [
    "print(tf.VERSION)\n",
    "print(tf.keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras can run any Keras-compatible code, but keep in mind:\n",
    "\n",
    "# The tf.keras version in the latest TensorFlow release might not be the same as the latest keras version from PyPI. Check tf.keras.version.\n",
    "# When saving a model's weights, tf.keras defaults to the checkpoint format. Pass save_format='h5' to use HDF5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build a simple model - Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In Keras, you assemble layers to build models.\n",
    "# A model is (usually) a graph of layers. The most common type of model is a stack of layers: the tf.keras.Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To build a simple, fully-connected network (i.e. multi-layer perceptron):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# Add another:\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "# Add a softmax layer with 10 output units:\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure the layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are many tf.keras.layers available with some common constructor parameters:\n",
    "\n",
    "#(1) activation: Set the activation function for the layer. This parameter is specified by the name of a built-in function \n",
    "#    or as a callable object. By default, no activation is applied.\n",
    "\n",
    "#(2) kernel_initializer and bias_initializer: The initialization schemes that create the layer's weights (kernel and bias). \n",
    "#    This parameter is a name or a callable object. This defaults to the \"Glorot uniform\" initializer.\n",
    "\n",
    "#(3) kernel_regularizer and bias_regularizer: The regularization schemes that apply the layer's weights (kernel and bias), \n",
    "#    such as L1 or L2 regularization. By default, no regularization is applied.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following instantiates tf.keras.layers.Dense layers using constructor arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1b853d544e0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a sigmoid layer:\n",
    "layers.Dense(64, activation='sigmoid')\n",
    "# Or:\n",
    "layers.Dense(64, activation=tf.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1b853d93080>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A linear layer with L1 regularization of factor 0.01 applied to the kernel matrix:\n",
    "layers.Dense(64, kernel_regularizer=tf.keras.regularizers.l1(0.01))\n",
    "\n",
    "# A linear layer with L2 regularization of factor 0.01 applied to the bias vector:\n",
    "layers.Dense(64, bias_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "\n",
    "# A linear layer with a kernel initialized to a random orthogonal matrix:\n",
    "layers.Dense(64, kernel_initializer='orthogonal')\n",
    "\n",
    "# A linear layer with a bias vector initialized to 2.0s:\n",
    "layers.Dense(64, bias_initializer=tf.keras.initializers.constant(2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "# Adds a densely-connected layer with 64 units to the model:\n",
    "layers.Dense(64, activation='relu', input_shape=(32,)),\n",
    "# Add another:\n",
    "layers.Dense(64, activation='relu'),\n",
    "# Add a softmax layer with 10 output units:\n",
    "layers.Dense(10, activation='softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After the model is constructed, configure its learning process by calling the compile method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.train.AdamOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.Model.compile takes three important arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) - optimizer: This object specifies the training procedure. Pass it optimizer instances from the tf.train module, \n",
    "#      such as tf.train.AdamOptimizer, tf.train.RMSPropOptimizer, or tf.train.GradientDescentOptimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2) - loss: The function to minimize during optimization. Common choices include mean square error (mse), categorical_crossentropy, \n",
    "#      and binary_crossentropy. Loss functions are specified by name or by passing a callable object from the tf.keras.losses module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(3) - metrics: Used to monitor training. These are string names or callables from the tf.keras.metrics module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The following shows a few examples of configuring a model for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a model for mean-squared error regression.\n",
    "model.compile(optimizer=tf.train.AdamOptimizer(0.01),\n",
    "              loss='mse',       # mean squared error\n",
    "              metrics=['mae'])  # mean absolute error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure a model for categorical classification.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),\n",
    "              loss=tf.keras.losses.categorical_crossentropy,\n",
    "              metrics=[tf.keras.metrics.categorical_accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input NumPy data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For small datasets, use in-memory NumPy arrays to train and evaluate a model. The model is \"fit\" to the training data using the fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.04707837, 0.70829699, 0.24101767, 0.9635169 , 0.33010842,\n",
       "        0.26891436, 0.60825772, 0.67567216, 0.06508767, 0.14278243,\n",
       "        0.01740133, 0.36742832, 0.53356225, 0.92267909, 0.33391516,\n",
       "        0.37151945, 0.18428788, 0.88881528, 0.84612336, 0.28030468,\n",
       "        0.09979304, 0.30345087, 0.15039404, 0.32016796, 0.57977302,\n",
       "        0.66528371, 0.47316572, 0.44634287, 0.80184547, 0.43621164,\n",
       "        0.24297385, 0.98913583],\n",
       "       [0.21337579, 0.36469759, 0.15376238, 0.09482156, 0.87239564,\n",
       "        0.36000593, 0.00447349, 0.10187727, 0.38651814, 0.72167006,\n",
       "        0.20170619, 0.3532711 , 0.98755966, 0.56195899, 0.25635043,\n",
       "        0.62563139, 0.98768382, 0.43287294, 0.96515528, 0.37713148,\n",
       "        0.24600196, 0.96502573, 0.56082465, 0.00345328, 0.54452404,\n",
       "        0.19855453, 0.95232094, 0.68969721, 0.78351173, 0.21658256,\n",
       "        0.64064648, 0.43199805],\n",
       "       [0.11151892, 0.03322179, 0.57154059, 0.60495043, 0.36705205,\n",
       "        0.96764649, 0.29063315, 0.67970432, 0.13859091, 0.71538511,\n",
       "        0.66364034, 0.32962038, 0.67214733, 0.53713801, 0.79112607,\n",
       "        0.63006772, 0.12944608, 0.09922131, 0.62127745, 0.46387945,\n",
       "        0.24934159, 0.70251727, 0.01942182, 0.83164007, 0.1752599 ,\n",
       "        0.93187917, 0.00831418, 0.4547857 , 0.03002213, 0.9898448 ,\n",
       "        0.02540875, 0.71143268],\n",
       "       [0.88817278, 0.59074592, 0.69786512, 0.99521437, 0.64456055,\n",
       "        0.18233051, 0.63660145, 0.40771638, 0.65726287, 0.73668693,\n",
       "        0.38593676, 0.72770164, 0.94695765, 0.44880786, 0.63777867,\n",
       "        0.22389117, 0.47881627, 0.46461985, 0.51906191, 0.47090025,\n",
       "        0.4632266 , 0.07328353, 0.59192255, 0.45499019, 0.83858486,\n",
       "        0.64826204, 0.49106519, 0.23475948, 0.12530185, 0.63089711,\n",
       "        0.10323091, 0.96594386],\n",
       "       [0.34934629, 0.53723143, 0.56983152, 0.02234998, 0.80598502,\n",
       "        0.57909061, 0.30805339, 0.44350975, 0.9267696 , 0.48431045,\n",
       "        0.54740204, 0.77793734, 0.00822996, 0.32739608, 0.71089652,\n",
       "        0.00176752, 0.14759857, 0.50507789, 0.6658175 , 0.24787653,\n",
       "        0.65040613, 0.62718703, 0.62643951, 0.09255494, 0.85206179,\n",
       "        0.31030717, 0.63076609, 0.83649391, 0.92629674, 0.93439587,\n",
       "        0.7810687 , 0.50719207]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1000/1000 [==============================] - 1s 606us/step - loss: 11.5646 - categorical_accuracy: 0.0970\n",
      "Epoch 2/10\n",
      "1000/1000 [==============================] - 0s 144us/step - loss: 11.5436 - categorical_accuracy: 0.0850\n",
      "Epoch 3/10\n",
      "1000/1000 [==============================] - 0s 145us/step - loss: 11.5354 - categorical_accuracy: 0.0910\n",
      "Epoch 4/10\n",
      "1000/1000 [==============================] - 1s 526us/step - loss: 11.5251 - categorical_accuracy: 0.1040TA: 0s - loss: 11.4143 - categorical_accuracy:\n",
      "Epoch 5/10\n",
      "1000/1000 [==============================] - 0s 280us/step - loss: 11.5254 - categorical_accuracy: 0.1140\n",
      "Epoch 6/10\n",
      "1000/1000 [==============================] - 0s 193us/step - loss: 11.5225 - categorical_accuracy: 0.1020\n",
      "Epoch 7/10\n",
      "1000/1000 [==============================] - 0s 189us/step - loss: 11.5206 - categorical_accuracy: 0.1310\n",
      "Epoch 8/10\n",
      "1000/1000 [==============================] - 0s 153us/step - loss: 11.5202 - categorical_accuracy: 0.1290\n",
      "Epoch 9/10\n",
      "1000/1000 [==============================] - 0s 162us/step - loss: 11.5134 - categorical_accuracy: 0.1360\n",
      "Epoch 10/10\n",
      "1000/1000 [==============================] - 0s 172us/step - loss: 11.5116 - categorical_accuracy: 0.1410\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b853d276a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, labels, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### tf.keras.Model.fit takes three important arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#(1) - epochs: Training is structured into epochs. An epoch is one iteration over the entire input data (this is done in smaller batches).\n",
    "#(2) - batch_size: When passed NumPy data, the model slices the data into smaller batches and iterates over these batches during training.\n",
    "#      This integer specifies the size of each batch. Be aware that the last batch may be smaller if the total number of samples is not divisible by the batch size.\n",
    "#(3) - validation_data: When prototyping a model, you want to easily monitor its performance on some validation data.\n",
    "#      Passing this argument—a tuple of inputs and labels—allows the model to display the loss and metrics in inference mode for the passed data, at the end of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's an example using validation_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "val_data = np.random.random((100, 32))\n",
    "val_labels = np.random.random((100, 10))\n",
    "\n",
    "model.fit(data, labels, epochs=10, batch_size=32,\n",
    "          validation_data=(val_data, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input tf.data datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Datasets API to scale to large datasets or multi-device training. Pass a tf.data.Dataset instance to the fit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a toy dataset instance:\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset.\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, the fit method uses the steps_per_epoch argument—this is the number of training steps the model runs before \n",
    "# it moves to the next epoch. Since the Dataset yields batches of data, this snippet does not require a batch_size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets can also be used for validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "dataset = dataset.batch(32).repeat()\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels))\n",
    "val_dataset = val_dataset.batch(32).repeat()\n",
    "\n",
    "model.fit(dataset, epochs=10, steps_per_epoch=30,\n",
    "          validation_data=val_dataset,\n",
    "          validation_steps=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate and predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tf.keras.Model.evaluate and tf.keras.Model.predict methods can use NumPy data and a tf.data.Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To evaluate the inference-mode loss and metrics for the data provided:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 132us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[11.574226791381836, 0.108]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))\n",
    "\n",
    "model.evaluate(data, labels, batch_size=32)\n",
    "\n",
    "#model.evaluate(dataset, steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And to predict the output of the last layer in inference for the data provided, as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.predict(data, batch_size=32)\n",
    "print(result.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the Models\n",
    "- Save and load the weights of a model using tf.keras.Model.save_weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save weights to a TensorFlow Checkpoint file\n",
    "model.save_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restore the model's state,\n",
    "# this requires a model with the same architecture.\n",
    "model_weight_load = model.load_weights('./weights/my_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the entire Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TensorFlow optimizers do not make it possible to access optimizer attributes or optimizer state after instantiation. As a result, we cannot save the optimizer as part of the model save file.You will have to compile your model again after loading it. Prefer using a Keras optimizer instead (see keras.io/optimizers).\n"
     ]
    }
   ],
   "source": [
    "# Save entire model to a HDF5 file\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file: the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# Recreate the exact same model, including weights and optimizer.\n",
    "model_loaded = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Predictions using saved models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.13718262, 0.08531354, 0.08952003, 0.092276  , 0.08132118,\n",
       "        0.11917246, 0.09612212, 0.09775688, 0.11063788, 0.0906973 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_loaded.predict(data[0:1], batch_size=32)\n",
    "predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functional API\n",
    "\n",
    "# The tf.keras.Sequential model is a simple stack of layers that cannot represent arbitrary models.\n",
    "# Use the Keras functional API to build complex model topologies such as:\n",
    "\n",
    "#   Multi-input models,\n",
    "#   Multi-output models,\n",
    "#   Models with shared layers (the same layer called several times),\n",
    "#   Models with non-sequential data flows (e.g. residual connections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a model with the functional API works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) A layer instance is callable and returns a tensor.\n",
    "# (2) Input tensors and output tensors are used to define a tf.keras.Model instance.\n",
    "# (3) This model is trained just like the Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example uses the functional API to build a simple, fully-connected network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=(32,))  # Returns a placeholder tensor\n",
    "\n",
    "# A layer instance is callable on a tensor, and returns a tensor.\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "predictions = layers.Dense(10, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model given inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model subclassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a fully-customizable model by subclassing tf.keras.Model and defining your own forward pass. \n",
    "# Create layers in the __init__ method and set them as attributes of the class instance. Define the forward pass in the call method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model subclassing is particularly useful when eager execution is enabled since the forward pass can be written imperatively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key Point: Use the right API for the job. While model subclassing offers flexibility, \n",
    "# it comes at a cost of greater complexity and more opportunities for user errors. If possible, prefer the functional API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following example shows a subclassed tf.keras.Model using a custom forward pass:\n",
    "\n",
    "class MyModel(tf.keras.Model):\n",
    "\n",
    "  def __init__(self, num_classes=10):\n",
    "    super(MyModel, self).__init__(name='my_model')\n",
    "    self.num_classes = num_classes\n",
    "    # Define your layers here.\n",
    "    self.dense_1 = layers.Dense(32, activation='relu')\n",
    "    self.dense_2 = layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "  def call(self, inputs):\n",
    "    # Define your forward pass here,\n",
    "    # using layers you previously defined (in `__init__`).\n",
    "    x = self.dense_1(inputs)\n",
    "    return self.dense_2(x)\n",
    "\n",
    "  def compute_output_shape(self, input_shape):\n",
    "    # You need to override this function if you want to use the subclassed model\n",
    "    # as part of a functional-style model.\n",
    "    # Otherwise, this method is optional.\n",
    "    shape = tf.TensorShape(input_shape).as_list()\n",
    "    shape[-1] = self.num_classes\n",
    "    return tf.TensorShape(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the new model class:\n",
    "\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Trains for 5 epochs.\n",
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and restore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Eager Execution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training a MNIST Model using Eager Execution \n",
    "# https://www.tensorflow.org/guide/eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
